{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> ***IMPORTING ALL THE NECESSARY LIBRARIES***(readme step 1) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request as request\n",
    "from skimage import io\n",
    "import sys\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2gray\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import re\n",
    "from datetime import date \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import heapq\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> **IMPORTING THE DATAFILES** (readme step 2) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('steam.csv')\n",
    "data2 = pd.read_csv('steam_media_data.csv')\n",
    "data3 = pd.read_csv('steam_description_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> ***PRE PROCESSING STEPS***</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> Creating empty columns in dataframe (readme step 3)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DLC'] =0 \n",
    "data['Action'] = 0\n",
    "data['In_app_purchase'] = 0\n",
    "data['Windows'] = 0\n",
    "data['MAC'] = 0\n",
    "data['Linux'] = 0\n",
    "data['total_ratings'] = 0\n",
    "data2['contrast_mean']=0.0\n",
    "data2[\"contrast_stdev\"]=0.0\n",
    "data2[\"contrast_median\"]=0.0\n",
    "data2[\"brightness\"]=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> creating total rating and positive rating percentage columsin the dataframe (readme step 4)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_ratings']= data['positive_ratings'] + data['negative_ratings']\n",
    "data['positive_percent']= data['positive_ratings']/data['total_ratings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> analysing reviews and owners (readme step 5) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datt2 = data.groupby('owners').mean()['total_ratings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "datt2.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> Converting oweners as a continuous variable Y (readme step 6)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in data.iterrows():\n",
    "    data.at[i,'lower_owners'] = int(row['owners'].split('-')[0])\n",
    "    data.at[i,'upper_owners'] = int(row['owners'].split('-')[1])\n",
    "df_max = pd.DataFrame(data.groupby('owners').max()['total_ratings'])\n",
    "df_min = pd.DataFrame(data.groupby('owners').min()['total_ratings'])\n",
    "df_count = pd.DataFrame(data.groupby('owners').count()['total_ratings'])\n",
    "data['y'] = 0\n",
    "for i, row in data.iterrows():\n",
    "    if(df_min.loc[row['owners']]['total_ratings'] == row['total_ratings']):\n",
    "        data.at[i,'y'] = row['lower_owners']\n",
    "    elif(df_max.loc[row['owners']]['total_ratings'] == row['total_ratings']):\n",
    "        data.at[i,'y'] = row['upper_owners']\n",
    "    else:\n",
    "        max_rev = df_max.loc[row['owners']]['total_ratings']\n",
    "        min_rev = df_min.loc[row['owners']]['total_ratings']\n",
    "        step = (row['upper_owners']-row['lower_owners'])/(max_rev - min_rev)\n",
    "        data.at[i,'y'] = row['lower_owners'] + step*(row['total_ratings'] - min_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>**SCRAPING** the data to find the Downloadable content presence in free game if any (readme step 7)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> using the url of games available in the data file, we scrap whether the game has downloadable content or not </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(datasetmod['appid'])):\n",
    "    \n",
    "    if data['price'][i] != 0.00:\n",
    "        data['DLC'][i] = 'NA'\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        inapp = []\n",
    "        \n",
    "        from selenium import webdriver\n",
    "        path=\"chromedriver.exe\"\n",
    "\n",
    "        import time\n",
    "        import random\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        chrome_options = Options()\n",
    "        #chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        \n",
    "        url = 'https://store.steampowered.com/app/' + str(dataset1['appid'][i]) + '/'\n",
    "        driver = webdriver.Chrome(path, options = chrome_options)\n",
    "        driver.get(url)\n",
    "\n",
    "        driver.title\n",
    "\n",
    "        # assert 'Team Fortress 2 on Steam' in driver.title\n",
    "        time.sleep(random.uniform(2,5))\n",
    "\n",
    "        # Scraping for dlc      \n",
    "\n",
    "        p1 = driver.find_elements_by_xpath('//*[@id=\"game_area_purchase\"]/h2')\n",
    "\n",
    "        for hamilton in p1:\n",
    "            inapp.append(hamilton.text)\n",
    "            \n",
    "        if inapp == []:\n",
    "            data['DLC'][i] = 0\n",
    "            \n",
    "        if inapp != []:\n",
    "            \n",
    "            data['DLC'][i] = 1\n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> Creating dummy variables for genres, In App Purchase and Platforms (readme step 8)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['appid'])):\n",
    "    \n",
    "    if 'Action' in data['genres'][i].split(';'):\n",
    "        data['Action'][i] = 1\n",
    "        \n",
    "    if 'In-App Purchases' in data['categories'][i].split(';'):\n",
    "        data['In_app_purchase'][i] = 1\n",
    "    \n",
    "    y = data['platforms'][i].split(';')\n",
    "    \n",
    "    if \"windows\" in y:\n",
    "        data['Windows'][i] = 1\n",
    "\n",
    "    if \"mac\" in y:\n",
    "        data['MAC'][i] = 1\n",
    "        \n",
    "    if \"linux\" in y:\n",
    "        data['Linux'][i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> **IMAGE PROCESSING** to generate brightness and contrast variables for game images (readme step 9)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> Reading all the image urls from the data2 file</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data2[\"header_image\"])):\n",
    "    start = data2[\"header_image\"][i].find(\"?\")\n",
    "    rem = data2[\"header_image\"][i][:start]\n",
    "    data2[\"header_image\"][i] = rem\n",
    "def load_image(url):\n",
    "    image_stream = request.urlopen(url)\n",
    "    return io.imread(image_stream, plugin='pil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> Calculating the contrast of every image in the data2 file</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import io\n",
    "for i in range(len(data2[\"header_image\"])):\n",
    "    img = io.imread(data2[\"header_image\"][i],0)\n",
    "    hist,bins = np.histogram(img.flatten(),256,[0,256])\n",
    "    cdf = hist.cumsum()\n",
    "    cdf_normalized = cdf * hist.max()/ cdf.max()\n",
    "    cdf_m = np.ma.masked_equal(cdf,0)\n",
    "    cdf_m = (cdf_m - cdf_m.min())*255/(cdf_m.max()-cdf_m.min())\n",
    "    cdf = np.ma.filled(cdf_m,0).astype('uint8')\n",
    "    stdev = np.std(cdf_m)\n",
    "    median = np.median(cdf_m)\n",
    "    mean = np.mean(cdf_m)\n",
    "    data2['contrast_mean'][i] = mean\n",
    "    data2[\"contrast_stdev\"][i]=stdev\n",
    "    data2[\"contrast_median\"][i]=median\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> Calculating the brightness of every image in the data2 file</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_brightness(image):\n",
    "    greyscale_image = image.convert('L')\n",
    "    histogram = greyscale_image.histogram()\n",
    "    pixels = sum(histogram)\n",
    "    brightness = scale = len(histogram)\n",
    "\n",
    "    for index in range(0, scale):\n",
    "        ratio = histogram[index] / pixels\n",
    "        brightness += ratio * (-scale + index)\n",
    "\n",
    "    return 1 if brightness == 255 else brightness / scale\n",
    "for i in range(len(data2[\"header_image\"])):\n",
    "    try:\n",
    "        response = requests.get(data2['header_image'][i])\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        data2[\"brightness\"][i]=calculate_brightness(image)\n",
    "    except OSError:\n",
    "        \n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>merging the two dataset using left join on data </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data,data2, left_on='appid', right_on='steam_appid' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> removing useless columns</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['english']==1]\n",
    "data = data.drop(['steam_api','header_image','screenshots','background','movies','total_ratings','lower_owners','upper_owners','categories','genres','steamspy_tags', 'positive_ratings','negative_ratings','average_playtime','platforms'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> Sentimental analysis and **WORD CLOUD** (readme step 10)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('steam.csv')\n",
    "dataa = pd.merge(data1,data3,left_on = 'appid', right_on = 'steam_appid')\n",
    "lines = []\n",
    "for i in range(len(dataa['appid'])):\n",
    "    \n",
    "    y = dataa['owners'][i].split('-')[0]  \n",
    "    if int(y) >= 100000:\n",
    "        \n",
    "        lines.append(dataa['detailed_description'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def striphtml(dat):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', dat)\n",
    "\n",
    "line_3=[]\n",
    "for j in range(len(lines)):\n",
    "    line_3.append(str.strip(striphtml(lines[j])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff= ''\n",
    "for i in range(len(line_3)):\n",
    "    line_3[i] = line_3[i].replace('quot','')\n",
    "    ff = ff+ line_3[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,STOPWORDS\n",
    "# Start with one review:\n",
    "text = ff\n",
    "\n",
    "new_stopwords = STOPWORDS\n",
    "\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(max_font_size=1000, max_words=50, background_color=\"white\",width=800, height=400, stopwords=new_stopwords, normalize_plurals= True).generate(text)\n",
    "# wordcloud.recolor(color_func = grey_color_func)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('word2.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> **NATURAL LANGUAGE PROCESSING** to find the popular topics in the description of games using **NLTK** on data3 file (readme step 11)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> **TOKENIZATION** and stop word removal</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3a['detailed_description'][1298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3a = pd.merge(data1,data3,left_on = 'appid', right_on = 'steam_appid')\n",
    "\n",
    "data3a = data3a[data3a['english'] == 1]\n",
    "\n",
    "lines = []\n",
    "for i,row in data3a.iterrows():\n",
    "    \n",
    "    lines.append(row['detailed_description'])\n",
    "\n",
    "import re\n",
    "def striphtml(dat):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', dat)\n",
    "\n",
    "line_3=[]\n",
    "for j in range(len(lines)):\n",
    "    line_3.append(str.strip(striphtml(lines[j])))\n",
    "\n",
    "#TOKENIZE\n",
    "\n",
    "token = []\n",
    "\n",
    "for j in range(len(line_3)):\n",
    "    token.append(nltk.word_tokenize(line_3[j]))\n",
    "\n",
    "#remove stop words\n",
    "    \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_removed=[]\n",
    "\n",
    "for m in range(len(token)):\n",
    "    stop_words_removed.append([t1 for t1 in token[m] if not t1 in stopwords.words('english') if t1.isalpha()])\n",
    "\n",
    "print(stop_words_removed)        \n",
    "                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> Lemmatization </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemm=[]\n",
    "\n",
    "for k in range(len(stop_words_removed)):\n",
    "    lemm.append([lemmatizer.lemmatize(t) for t in stop_words_removed[k] if t.isalpha()])\n",
    "             \n",
    "    \n",
    "# reviews in Count vectors\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stop=[]\n",
    "for j in range(len(lemm)):\n",
    "    s=\" \".join(stop_words_removed[j])\n",
    "    stop.append(s)\n",
    "\n",
    "vectorizer2 = CountVectorizer(ngram_range=(1,2),min_df =5,max_df=1000)\n",
    "vectorizer2.fit(stop)\n",
    "print(vectorizer2.vocabulary_)\n",
    "v1 = vectorizer2.transform(stop)\n",
    "print(v1.toarray())\n",
    "\n",
    "\n",
    "# reviews in Term Frequency-Inverse Document Frequency Vectors\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stop=[]\n",
    "for j in range(len(stop_words_removed)):\n",
    "    s=\" \".join(stop_words_removed[j])\n",
    "    stop.append(s)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2),min_df =5,max_df=1000)\n",
    "vectorizer.fit(stop)\n",
    "print(vectorizer.vocabulary_)\n",
    "v = vectorizer.transform(stop)\n",
    "print(v.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'> **Latent Dirichlet allocation** model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Model-Fit\n",
    "\n",
    "terms = vectorizer2.get_feature_names()\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=6, max_iter=20, learning_method='online', learning_offset=50.,random_state=0).fit(v1)\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx))\n",
    "    print(\" \".join([terms[i] for i in topic.argsort()[:-4-1:-1]]))\n",
    "    \n",
    "\n",
    "# LDA Model - Transformation\n",
    "topics=[]\n",
    "for i in range(len(df_2)):\n",
    "    topics.append(lda.transform(v1[i])[0])\n",
    "\n",
    "topic_dist = lda.transform(v1)\n",
    "\n",
    "df_topic=pd.DataFrame(topic_dist)\n",
    "\n",
    "df_topic.to_csv('Topics.csv', header=True, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
